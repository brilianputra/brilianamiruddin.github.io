---
layout: post
title: "[Optimal Control/최적제어] Model Predictive Path Integral Control (MPPI) (ICRA 2016)"
toc: true
mathjax: true
categories: study-log
author:
  - Brilian Putra Amiruddin
tags: [optimal_control,optimization,mppi,learning]
--- 
# Model Predictive Path Integral Control (MPPI) (ICRA 2016)

The path integral control framework is a way to solve an optimal control problem by generating stochastic samples of the trajectories. In path integral control, the value function of the optimal control problem is transformed into a path integral, which is an expectation over all possible trajectories. The path integral allows the stochastic optimal control problem to be solved with a Monte Carlo approximation.

In previous works before MPPI, there were various algorithms in the path integral control setting (e.g., model predictive control, policy improvement with Path Integrals, and Iterative Feedback). However, those algorithms are limited in their behavior and do not consider the full Nonlinearity of Dynamics.

Another efficient and better approach is to use a GPU to exploit the parallel nature of sampling so that thousands of samples of trajectories from nonlinear dynamics can be sampled without a problem. Nevertheless, the expectation is taken with respect to the uncontrolled dynamics (u=0), which means that the probability of sampling a low-cost trajectory is technically very low.

## Core Ideas of MPPI (from the Learning to Control 2021 Tutorial):

-   Based on Model Predictive Control (MPC)
-   Simulate into the future (running thousands of rollouts
-   From the rollout results, which have randomly different inputs (we can understand whether it is good or bad)
-   The best input will be the weighted sum of inputs
    -   The rollout has low cost → larger weights
-   Update inputs and repeat again

## The MPPI Algorithm

![MPPI Algorithm](/assets/fig/MPPI.png)

## Explanation of the Algorithm
$\text{Define your horizon steps and the number of trajectory samples (line 1 and 2)}$\
$\text{Initialize control sequence (line 3)}$ \
$\text{while task not completed, do:}$ \
$\text{Generate random pertubations} \hspace{1mm} \delta_u$ \
$\hspace{2mm} \text{for Monte Carlo rollouts k=1...K do:}$ \
$\hspace{6mm} \text{Start in current state} x_{k,0}=x(t_0)$ \
$\hspace{6mm} \text{for MPC horizon steps n=0..N-1 do:}$ \
$\hspace{8mm} \text{Input} \, u_{k,n} = u_n + \delta u_{k,n}$ \
$\hspace{8mm} \text{Next state} \, x_{k,n+1} = model(x_{k,n}, u_{k,n})$ \
$\hspace{8mm} \text{Rollout cost} \, S_k = S_k + stage cost  \, q_{k,n}$ \
$\hspace{6mm} \text{end}$ \
$\hspace{2mm} \text{end}$ \
$\hspace{2mm} \text{for n=0 .. N-1 do:}$ \
$\hspace{6mm} u_n = u_n + \text{reward weighted perturbations}$ \
$\hspace{2mm} \text{end}$ \
$\hspace{2mm} \text{Apply first input} \, u_0$ \
$\hspace{2mm} \text{Get system feedback}$ \
$\hspace{2mm} \text{Check if the task completed}$ \
$\text{end}$

![MPPI](/assets/fig/MPPI2.png)

Let’s say we are going to define the MPC with $N=5$ horizon steps (line 2)
With $K=4$ samples of random disturbance vectors (line 1) 
$$\delta u_k \in  \mathbb{R}^5$$ 
Then, we are going to have 4 rollouts (line 8)

$$\cdot  \cdot  \dots  \cdot  \cdot  \delta u_1 \\  \cdot  \cdot  \dots  \cdot  \cdot  \delta u_2 \\  \cdot  \cdot  \dots  \cdot  \cdot  \delta u_3 \\  \cdot  \cdot  \dots  \cdot  \cdot  \delta u_4$$
In rollout k, apply the disturbed input vector within the horizon length (line 15)
$$u + \delta u_k \in  \mathbb{R}^5, \text{u is the nominal input}$$
Updating the nominal input with reward weighted perturbations $$u = u + \frac{\sum{w_k \delta u_k}}{\sum w_k}$$

Where the weights are $$w_k = e^{-\frac{1}{\lambda}S_k}$$

$$ S_k: \text{\, cost of trajectory k}  \\  \lambda: \,  \text{constant parameter} $$

References:
-   [Learning to Control 2021 Tutorial](https://www.youtube.com/watch?v=19QLyMuQ_BE)
-   G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou, "Aggressive driving with model predictive path integral control," 2016 IEEE International Conference on Robotics and Automation (ICRA)
-   [Model Predictive Path Integral Control: From Theory to Parallel Computation](https://arc.aiaa.org/doi/abs/10.2514/1.G001921) Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou, Journal of Guidance, Control, and Dynamics 2017

